Q: What is feature selection in machine learning?
A: Feature selection is the process of selecting a subset of relevant features for model building to improve performance and reduce overfitting.

Q: Explain wrapper methods for feature selection.
A: Wrapper methods evaluate multiple subsets of features by training a model on each subset to determine the optimal combination for the model.

Q: What is the difference between filtering and embedded methods for feature selection?
A: Filtering methods remove irrelevant features before model training, while embedded methods perform feature selection during the training process, such as decision trees selecting features.

Q: What is feature engineering?
A: Feature engineering involves creating meaningful input variables from raw data to improve the performance of machine learning models.

Q: How does log transformation benefit numeric features?
A: Log transformation reduces skewness in data and brings features closer to a normal distribution, which helps in linear models.

Q: What is the purpose of scaling numeric features?
A: Scaling normalizes features to ensure they have similar ranges, which is important for distance-based algorithms like k-means and k-nearest neighbors.

Q: Describe the bag-of-words model in text processing.
A: The bag-of-words model converts text data into a matrix of word counts, ignoring grammar and word order, useful for text classification tasks.

Q: What is TF-IDF, and how does it differ from the bag-of-words model?
A: TF-IDF adjusts word counts by giving less weight to common words and more weight to rare, informative ones, improving performance in text classification.

Q: What are interaction features, and when should they be used?
A: Interaction features combine two or more features to capture relationships between them, often used when the interaction between variables affects the target.

Q: What is feature hashing, and what are its trade-offs?
A: Feature hashing maps high-dimensional data into a lower-dimensional space using hash functions, reducing memory usage but potentially introducing collisions.

Q: How do decision trees handle categorical features?
A: Decision trees can directly split data based on categorical features, making them well-suited for tasks involving non-numeric data.

Q: Explain PCA and its role in dimensionality reduction.
A: Principal Component Analysis (PCA) reduces the number of features by transforming them into a set of uncorrelated components, retaining as much variance as possible.

Q: What is k-means clustering?
A: K-means is an unsupervised learning algorithm that groups similar data points into k clusters based on feature similarity.

Q: How does cross-validation help in model evaluation?
A: Cross-validation splits the dataset into training and validation sets multiple times to ensure that the model generalizes well to unseen data.

Q: What is the purpose of boosting in ensemble learning?
A: Boosting trains multiple weak models sequentially, with each model correcting the errors of the previous one, to improve overall performance.

Q: Describe the bias-variance tradeoff in machine learning.
A: The bias-variance tradeoff involves balancing a modelâ€™s complexity: too simple (high bias) leads to underfitting, while too complex (high variance) leads to overfitting.

Q: How is LASSO regression used for feature selection?
A: LASSO regression adds an L1 penalty to the loss function, shrinking coefficients of less important features to zero, effectively selecting a subset of features.

Q: What is the role of deep learning in feature extraction?
A: Deep learning automatically extracts high-level features from raw data, especially useful in complex tasks like image and text analysis.

Q: What are the strengths and weaknesses of k-nearest neighbors (kNN)?
A: kNN is simple and effective for classification, but it requires large memory and can be slow, especially for large datasets, since it computes distances for all training examples.

Q: How do convolutional neural networks (CNNs) handle image features?
A: CNNs use convolutional layers to automatically learn spatial hierarchies in images, making them highly effective for image recognition tasks.
